input {
    kafka {
        zk_connect => "Collector01:2181,Collector02:2181,Collector03:2181,Collector04:2181"
        topic_id => '514'
    }
}

filter {
    mutate {
        remove_field => [ "host" ]
    }
    grok {
        match => ["source", "\/home\/softnixlogger\/groups\/%{INT:PortID}\/%{IPV4:sourcedata}\/%{INT:[@metadata][year]}-%{INT:[@metadata][month]}-%{INT:[@metadata][day]}\/%{INT:[@metadata][hour]}\/%{DATA:Fac}\/%{DATA:Pri}\.log"]
    }
    grok {
        match => ["message", "%{SYSLOGTIMESTAMP:log_timestamp} %{IPORHOST:host} %{SYSLOGPROG}: %{GREEDYDATA:raw_message}"]
    }
    date {
        match => [ "log_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        locale => "en"
        target => "received_at"
    }
    mutate {
        remove_field => [ "log_timestamp" ]
    }
}


output {
    #stdout { codec => rubydebug }
    elasticsearch {
        hosts => ["Logger01:9200","Logger02:9200"]
        codec => "plain"
        workers => 1
        index => "logstash-%{+YYYY.MM.dd}"
        manage_template => true
        template_name => "logstash"
        template_overwrite => false
        flush_size => 500
        idle_flush_time => 1
    }

    file {
        codec => "plain"
        message_format => "%{message}"
        path => ["/home/softnixlogger/groups/514/%{sourcedata}/%{[@metadata][year]}-%{[@metadata][month]}-%{[@metadata][day]}/%{[@metadata][hour]}/%{Fac}/%{Pri}.log"]
    }

    file {
        codec => "plain"
        message_format => "."
        path => ["/home/data/dev/514/%{sourcedata}/%{Fac}/%{Pri}"]
    }

    if ("_grokparsefailure" in [tags]) {
        file {
            codec => "plain"
            message_format => "%{message}"
            path => ["/home/softnixlogger/grokfailed/514/%{sourcedata}/%{[@metadata][year]}-%{[@metadata][month]}-%{[@metadata][day]}/%{[@metadata][hour]}/%{Fac}/%{Pri}.log"]
        }
    }
}



